{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOz7tluV5IM6G4qYTC1B/Ut",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sbagency/colab-hacks/blob/main/Llam3_2_Groq_Inference_Chainlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chainlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iQftdqs3bwk",
        "outputId": "067cb1b5-dcb0-4896-a7fa-b2b46dd79346"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chainlit in /usr/local/lib/python3.10/dist-packages (1.2.0)\n",
            "Requirement already satisfied: aiofiles<24.0.0,>=23.1.0 in /usr/local/lib/python3.10/dist-packages (from chainlit) (23.2.1)\n",
            "Requirement already satisfied: asyncer<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from chainlit) (0.0.7)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from chainlit) (8.1.7)\n",
            "Requirement already satisfied: dataclasses_json<0.7.0,>=0.6.7 in /usr/local/lib/python3.10/dist-packages (from chainlit) (0.6.7)\n",
            "Requirement already satisfied: fastapi<0.113,>=0.110.1 in /usr/local/lib/python3.10/dist-packages (from chainlit) (0.112.4)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chainlit) (1.2.0)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from chainlit) (0.27.2)\n",
            "Requirement already satisfied: lazify<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from chainlit) (0.4.0)\n",
            "Requirement already satisfied: literalai==0.0.607 in /usr/local/lib/python3.10/dist-packages (from chainlit) (0.0.607)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from chainlit) (1.6.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.26 in /usr/local/lib/python3.10/dist-packages (from chainlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<24.0,>=23.1 in /usr/local/lib/python3.10/dist-packages (from chainlit) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from chainlit) (2.9.2)\n",
            "Requirement already satisfied: pyjwt<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from chainlit) (2.9.0)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from chainlit) (1.0.1)\n",
            "Requirement already satisfied: python-multipart<0.0.10,>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from chainlit) (0.0.9)\n",
            "Requirement already satisfied: python-socketio<6.0.0,>=5.11.0 in /usr/local/lib/python3.10/dist-packages (from chainlit) (5.11.4)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from chainlit) (0.37.2)\n",
            "Requirement already satisfied: syncer<3.0.0,>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from chainlit) (2.0.3)\n",
            "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from chainlit) (2.0.1)\n",
            "Requirement already satisfied: uptrace<2.0.0,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from chainlit) (1.26.0)\n",
            "Requirement already satisfied: uvicorn<0.26.0,>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from chainlit) (0.25.0)\n",
            "Requirement already satisfied: watchfiles<0.21.0,>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from chainlit) (0.20.0)\n",
            "Requirement already satisfied: chevron>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from literalai==0.0.607->chainlit) (0.14.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from asyncer<0.0.8,>=0.0.7->chainlit) (3.7.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses_json<0.7.0,>=0.6.7->chainlit) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses_json<0.7.0,>=0.6.7->chainlit) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi<0.113,>=0.110.1->chainlit) (4.12.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->chainlit) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->chainlit) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->chainlit) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->chainlit) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.23.0->chainlit) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->chainlit) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->chainlit) (2.23.4)\n",
            "Requirement already satisfied: bidict>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from python-socketio<6.0.0,>=5.11.0->chainlit) (0.23.1)\n",
            "Requirement already satisfied: python-engineio>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from python-socketio<6.0.0,>=5.11.0->chainlit) (4.9.1)\n",
            "Requirement already satisfied: opentelemetry-api~=1.26 in /usr/local/lib/python3.10/dist-packages (from uptrace<2.0.0,>=1.22.0->chainlit) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp~=1.26 in /usr/local/lib/python3.10/dist-packages (from uptrace<2.0.0,>=1.22.0->chainlit) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation~=0.47b0 in /usr/local/lib/python3.10/dist-packages (from uptrace<2.0.0,>=1.22.0->chainlit) (0.48b0)\n",
            "Requirement already satisfied: opentelemetry-sdk~=1.26 in /usr/local/lib/python3.10/dist-packages (from uptrace<2.0.0,>=1.22.0->chainlit) (1.27.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.4.0->asyncer<0.0.8,>=0.0.7->chainlit) (1.2.2)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api~=1.26->uptrace<2.0.0,>=1.22.0->chainlit) (1.2.14)\n",
            "Requirement already satisfied: importlib-metadata<=8.4.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api~=1.26->uptrace<2.0.0,>=1.22.0->chainlit) (8.4.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.27.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp~=1.26->uptrace<2.0.0,>=1.22.0->chainlit) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http==1.27.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp~=1.26->uptrace<2.0.0,>=1.22.0->chainlit) (1.27.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.27.0->opentelemetry-exporter-otlp~=1.26->uptrace<2.0.0,>=1.22.0->chainlit) (1.65.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.27.0->opentelemetry-exporter-otlp~=1.26->uptrace<2.0.0,>=1.22.0->chainlit) (1.64.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.27.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.27.0->opentelemetry-exporter-otlp~=1.26->uptrace<2.0.0,>=1.22.0->chainlit) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.27.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.27.0->opentelemetry-exporter-otlp~=1.26->uptrace<2.0.0,>=1.22.0->chainlit) (1.27.0)\n",
            "Requirement already satisfied: requests~=2.7 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-http==1.27.0->opentelemetry-exporter-otlp~=1.26->uptrace<2.0.0,>=1.22.0->chainlit) (2.32.3)\n",
            "Requirement already satisfied: protobuf<5.0,>=3.19 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-proto==1.27.0->opentelemetry-exporter-otlp-proto-grpc==1.27.0->opentelemetry-exporter-otlp~=1.26->uptrace<2.0.0,>=1.22.0->chainlit) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation~=0.47b0->uptrace<2.0.0,>=1.22.0->chainlit) (71.0.4)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation~=0.47b0->uptrace<2.0.0,>=1.22.0->chainlit) (1.16.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk~=1.26->uptrace<2.0.0,>=1.22.0->chainlit) (0.48b0)\n",
            "Requirement already satisfied: simple-websocket>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from python-engineio>=4.8.0->python-socketio<6.0.0,>=5.11.0->chainlit) (1.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses_json<0.7.0,>=0.6.7->chainlit) (1.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api~=1.26->uptrace<2.0.0,>=1.22.0->chainlit) (3.20.2)\n",
            "Requirement already satisfied: wsproto in /usr/local/lib/python3.10/dist-packages (from simple-websocket>=0.10.0->python-engineio>=4.8.0->python-socketio<6.0.0,>=5.11.0->chainlit) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.27.0->opentelemetry-exporter-otlp~=1.26->uptrace<2.0.0,>=1.22.0->chainlit) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.27.0->opentelemetry-exporter-otlp~=1.26->uptrace<2.0.0,>=1.22.0->chainlit) (2.2.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pmr48jCc_T7q",
        "outputId": "ab348453-a662-4563-8989-70edb41b2411"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.50.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.5.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['GROQ_API_KEY'] = userdata.get('GROQ_API_KEY')"
      ],
      "metadata": {
        "id": "_OdgEYhE_Z0S"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RLJXSlQ8ly1e"
      },
      "outputs": [],
      "source": [
        "import portpicker\n",
        "port = portpicker.pick_unused_port()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#print(port)\n",
        "os.environ['PICKED_PORT'] = f\"{port}\"\n",
        "!echo $PICKED_PORT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gWbI16b7WFi",
        "outputId": "224b7746-4c8f-4846-c42c-26d596f9f1bc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39651\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.serve_kernel_port_as_window(port)\n",
        "output.serve_kernel_port_as_iframe(port)\n",
        "\n",
        "# get_ipython().system_raw(\"python3 -m http.server 8888 &\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "IPbDlBqb2i-h",
        "outputId": "b7a4b1b6-74ec-4963-b357-68ceae88e3c9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mWarning: This function may stop working due to changes in browser security.\n",
            "Try `serve_kernel_port_as_iframe` instead. \u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(39651, \"/\", \"https://localhost:39651/\", window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "    const iframe = document.createElement('iframe');\n",
              "    iframe.src = new URL(path, url).toString();\n",
              "    iframe.height = height;\n",
              "    iframe.width = width;\n",
              "    iframe.style.border = 0;\n",
              "    iframe.allow = [\n",
              "        'accelerometer',\n",
              "        'autoplay',\n",
              "        'camera',\n",
              "        'clipboard-read',\n",
              "        'clipboard-write',\n",
              "        'gyroscope',\n",
              "        'magnetometer',\n",
              "        'microphone',\n",
              "        'serial',\n",
              "        'usb',\n",
              "        'xr-spatial-tracking',\n",
              "    ].join('; ');\n",
              "    element.appendChild(iframe);\n",
              "  })(39651, \"/\", \"100%\", \"400\", false, window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#response = await client.chat.completions.create(\n",
        "#            model=\"gpt-4\",\n",
        "#            messages=messages,\n",
        "#            tools=functions,\n",
        "#            temperature=0.0,\n",
        "#            tool_choice=None\n",
        "#        )\n",
        "\n",
        "#!python"
      ],
      "metadata": {
        "id": "1fh5G7cT_ezh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de17c821-5334-4cf4-fbb3-8a9c155fa300"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0] on linux\n",
            "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
            ">>> l=[1,2,3,4,5]\n",
            ">>> del l[1:3]\n",
            ">>> l\n",
            "[1, 4, 5]\n",
            ">>> \n",
            "KeyboardInterrupt\n",
            ">>> \n",
            "KeyboardInterrupt\n",
            ">>> \n",
            "KeyboardInterrupt\n",
            ">>> ^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import openai\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "client = openai.AsyncOpenAI(\n",
        "  #base_url = \"https://integrate.api.nvidia.com/v1\",\n",
        "  #api_key = userdata.get('NVIDIA_API_KEY')\n",
        "  base_url=\"https://api.groq.com/openai/v1\",\n",
        "  api_key=os.environ['GROQ_API_KEY'] #userdata.get('GROQ_API_KEY')\n",
        ")\n",
        "\n",
        "#model = \"meta/llama-3.1-405b-instruct\"\n",
        "#model = \"llama-3.1-70b-versatile\"\n",
        "model = \"llama-3.2-90b-text-preview\"\n",
        "temperature=1\n",
        "top_p=1\n",
        "max_tokens=4096\n",
        "\n",
        "async def llmm(messages):\n",
        "  #messages=[{\"role\": \"user\",\"content\": prompt}]\n",
        "  completion = await client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=messages,\n",
        "  temperature=temperature,\n",
        "  top_p=top_p,\n",
        "  max_tokens=max_tokens,\n",
        "  stream=False\n",
        "  )\n",
        "  return completion.choices[0].message.content\n",
        "\n",
        "async def llm(prompt):\n",
        "  messages=[{\"role\": \"user\",\"content\": prompt}]\n",
        "  completion = await client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=messages,\n",
        "  temperature=temperature,\n",
        "  top_p=top_p,\n",
        "  max_tokens=max_tokens,\n",
        "  stream=False\n",
        "  )\n",
        "  return completion.choices[0].message.content\n",
        "\n",
        "import chainlit as cl\n",
        "\n",
        "system_message={\"role\": \"system\",\"content\": \"You are a helpful assistant!\"}\n",
        "\n",
        "@cl.on_chat_start\n",
        "async def on_chat_start():\n",
        "    cl.user_session.set(\"messages\", [system_message])\n",
        "\n",
        "MSG_MAX=3*2+1\n",
        "\n",
        "@cl.on_message\n",
        "async def main(message: cl.Message):\n",
        "    # Your custom logic goes here...\n",
        "    messages = cl.user_session.get(\"messages\")\n",
        "    messages.append({\"role\": \"user\",\"content\": message.content})\n",
        "    resp=await llmm(messages)\n",
        "    messages.append({\"role\": \"assistant\",\"content\": resp})\n",
        "    if len(messages)>MSG_MAX:\n",
        "      messages=[system_message]+messages[3:]\n",
        "    print(len(messages),messages)\n",
        "    cl.user_session.set(\"messages\", messages)\n",
        "    # Send a response back to the user\n",
        "    await cl.Message(\n",
        "        content=resp\n",
        "    ).send()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2V5Gxg1V7yHC",
        "outputId": "ad4ad4f6-3a33-424f-d5e3-64e2e2f3e2d3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.output import eval_js\n",
        "external_url=eval_js(f\"google.colab.kernel.proxyPort({port})\")\n",
        "print(external_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SBfwWiQr-yKU",
        "outputId": "c8fe77e7-ed99-4147-f6a8-2a529a614053"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://dt0fe3mg09w-496ff2e9c6d22116-39651-colab.googleusercontent.com/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $PICKED_PORT\n",
        "!chainlit run --port $PICKED_PORT app.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SqQVIkw6zMl",
        "outputId": "00b69ebd-ab1a-40e2-90f8-4822db33ea89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41117\n",
            "2024-09-30 09:28:43 - Your app is available at http://localhost:41117\n",
            "2024-09-30 09:28:50 - Translated markdown file for en-US not found. Defaulting to chainlit.md.\n",
            "2024-09-30 09:28:56 - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "3 [{'role': 'system', 'content': 'You are a helpful assistant!'}, {'role': 'user', 'content': 'message 1'}, {'role': 'assistant', 'content': \"I'd be happy to help. What's on your mind? Do you have a question, or is there something specific you'd like to know or discuss?\"}]\n",
            "2024-09-30 09:29:03 - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "5 [{'role': 'system', 'content': 'You are a helpful assistant!'}, {'role': 'user', 'content': 'message 1'}, {'role': 'assistant', 'content': \"I'd be happy to help. What's on your mind? Do you have a question, or is there something specific you'd like to know or discuss?\"}, {'role': 'user', 'content': 'message 2'}, {'role': 'assistant', 'content': 'It seems you sent a blank message or just the words \"message 1\" and \"message 2\". Could you please provide more context or information about what you need help with? I\\'m here to assist you, and I\\'d be happy to help once I know what you\\'re looking for.'}]\n",
            "2024-09-30 09:29:10 - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "7 [{'role': 'system', 'content': 'You are a helpful assistant!'}, {'role': 'user', 'content': 'message 1'}, {'role': 'assistant', 'content': \"I'd be happy to help. What's on your mind? Do you have a question, or is there something specific you'd like to know or discuss?\"}, {'role': 'user', 'content': 'message 2'}, {'role': 'assistant', 'content': 'It seems you sent a blank message or just the words \"message 1\" and \"message 2\". Could you please provide more context or information about what you need help with? I\\'m here to assist you, and I\\'d be happy to help once I know what you\\'re looking for.'}, {'role': 'user', 'content': 'message 3'}, {'role': 'assistant', 'content': \"It seems we're having a series of blank messages. If you're just testing the conversation, I'm happy to respond and let you know I'm here and working. However, if there's something specific on your mind, please feel free to share it with me, and I'll do my best to help.\"}]\n",
            "2024-09-30 09:29:14 - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "7 [{'role': 'system', 'content': 'You are a helpful assistant!'}, {'role': 'user', 'content': 'message 2'}, {'role': 'assistant', 'content': 'It seems you sent a blank message or just the words \"message 1\" and \"message 2\". Could you please provide more context or information about what you need help with? I\\'m here to assist you, and I\\'d be happy to help once I know what you\\'re looking for.'}, {'role': 'user', 'content': 'message 3'}, {'role': 'assistant', 'content': \"It seems we're having a series of blank messages. If you're just testing the conversation, I'm happy to respond and let you know I'm here and working. However, if there's something specific on your mind, please feel free to share it with me, and I'll do my best to help.\"}, {'role': 'user', 'content': 'message 4'}, {'role': 'assistant', 'content': \"It looks like we're stuck in a loop. If you're ready to break the cycle and start a real conversation, I'm here to listen and help. What's been on your mind lately? Do you have a question, a topic you'd like to discuss, or just need some assistance?\"}]\n",
            "2024-09-30 09:29:17 - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "7 [{'role': 'system', 'content': 'You are a helpful assistant!'}, {'role': 'user', 'content': 'message 3'}, {'role': 'assistant', 'content': \"It seems we're having a series of blank messages. If you're just testing the conversation, I'm happy to respond and let you know I'm here and working. However, if there's something specific on your mind, please feel free to share it with me, and I'll do my best to help.\"}, {'role': 'user', 'content': 'message 4'}, {'role': 'assistant', 'content': \"It looks like we're stuck in a loop. If you're ready to break the cycle and start a real conversation, I'm here to listen and help. What's been on your mind lately? Do you have a question, a topic you'd like to discuss, or just need some assistance?\"}, {'role': 'user', 'content': 'message 5'}, {'role': 'assistant', 'content': \"It seems like we're having a bit of fun with these numbered messages. If you're not ready to share a specific topic or question yet, I can try to spark some conversation. Would you like to talk about a hobby, a favorite TV show or movie, or maybe play a game? Just let me know, and I'll do my best to engage with you.\"}]\n",
            "2024-09-30 09:29:40 - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "7 [{'role': 'system', 'content': 'You are a helpful assistant!'}, {'role': 'user', 'content': 'message 4'}, {'role': 'assistant', 'content': \"It looks like we're stuck in a loop. If you're ready to break the cycle and start a real conversation, I'm here to listen and help. What's been on your mind lately? Do you have a question, a topic you'd like to discuss, or just need some assistance?\"}, {'role': 'user', 'content': 'message 5'}, {'role': 'assistant', 'content': \"It seems like we're having a bit of fun with these numbered messages. If you're not ready to share a specific topic or question yet, I can try to spark some conversation. Would you like to talk about a hobby, a favorite TV show or movie, or maybe play a game? Just let me know, and I'll do my best to engage with you.\"}, {'role': 'user', 'content': 'messge 6'}, {'role': 'assistant', 'content': \"Another message in the series. I'm starting to think this might be a record for the most numbered messages in a row with no actual conversation. Don't get me wrong, I'm happy to keep responding and hope that eventually, we'll break into a real conversation. If you need a prompt to get started, I can offer a fun question: If you could travel anywhere in the world right now, where would you go?\"}]\n",
            "2024-09-30 09:29:45 - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "7 [{'role': 'system', 'content': 'You are a helpful assistant!'}, {'role': 'user', 'content': 'message 5'}, {'role': 'assistant', 'content': \"It seems like we're having a bit of fun with these numbered messages. If you're not ready to share a specific topic or question yet, I can try to spark some conversation. Would you like to talk about a hobby, a favorite TV show or movie, or maybe play a game? Just let me know, and I'll do my best to engage with you.\"}, {'role': 'user', 'content': 'messge 6'}, {'role': 'assistant', 'content': \"Another message in the series. I'm starting to think this might be a record for the most numbered messages in a row with no actual conversation. Don't get me wrong, I'm happy to keep responding and hope that eventually, we'll break into a real conversation. If you need a prompt to get started, I can offer a fun question: If you could travel anywhere in the world right now, where would you go?\"}, {'role': 'user', 'content': 'message 7'}, {'role': 'assistant', 'content': 'I\\'m starting to feel like I\\'m in a repetitive loop. But don\\'t worry, I\\'ll keep responding until we break through to a real conversation. If you\\'re not ready to share your thoughts or respond yet, I can try to entertain you. Here\\'s a fun fact: did you know that there\\'s a species of jellyfish that\\'s immortal? Okay, maybe that\\'s not entirely accurate, but the Turritopsis dohrnii, also known as the \"immortal jellyfish,\" can transform its body into a younger state through a process called transdifferentiation. If you\\'re curious, I can tell you more about this fascinating creature.'}]\n",
            "2024-09-30 09:29:50 - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "7 [{'role': 'system', 'content': 'You are a helpful assistant!'}, {'role': 'user', 'content': 'messge 6'}, {'role': 'assistant', 'content': \"Another message in the series. I'm starting to think this might be a record for the most numbered messages in a row with no actual conversation. Don't get me wrong, I'm happy to keep responding and hope that eventually, we'll break into a real conversation. If you need a prompt to get started, I can offer a fun question: If you could travel anywhere in the world right now, where would you go?\"}, {'role': 'user', 'content': 'message 7'}, {'role': 'assistant', 'content': 'I\\'m starting to feel like I\\'m in a repetitive loop. But don\\'t worry, I\\'ll keep responding until we break through to a real conversation. If you\\'re not ready to share your thoughts or respond yet, I can try to entertain you. Here\\'s a fun fact: did you know that there\\'s a species of jellyfish that\\'s immortal? Okay, maybe that\\'s not entirely accurate, but the Turritopsis dohrnii, also known as the \"immortal jellyfish,\" can transform its body into a younger state through a process called transdifferentiation. If you\\'re curious, I can tell you more about this fascinating creature.'}, {'role': 'user', 'content': 'messsaage 8'}, {'role': 'assistant', 'content': \"I think we've set a new record for most consecutive numbered messages. Don't worry, I'm not losing my patience. I'm here to help and conversationally engage with you whenever you're ready. Since we're having fun with numbers, let's try a little math puzzle. If you have 8 apples and you give 2 to a friend, how many apples do you have left? (Just a gentle nudge to get us talking...)\"}]\n",
            "2024-09-30 09:30:05 - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "7 [{'role': 'system', 'content': 'You are a helpful assistant!'}, {'role': 'user', 'content': 'message 7'}, {'role': 'assistant', 'content': 'I\\'m starting to feel like I\\'m in a repetitive loop. But don\\'t worry, I\\'ll keep responding until we break through to a real conversation. If you\\'re not ready to share your thoughts or respond yet, I can try to entertain you. Here\\'s a fun fact: did you know that there\\'s a species of jellyfish that\\'s immortal? Okay, maybe that\\'s not entirely accurate, but the Turritopsis dohrnii, also known as the \"immortal jellyfish,\" can transform its body into a younger state through a process called transdifferentiation. If you\\'re curious, I can tell you more about this fascinating creature.'}, {'role': 'user', 'content': 'messsaage 8'}, {'role': 'assistant', 'content': \"I think we've set a new record for most consecutive numbered messages. Don't worry, I'm not losing my patience. I'm here to help and conversationally engage with you whenever you're ready. Since we're having fun with numbers, let's try a little math puzzle. If you have 8 apples and you give 2 to a friend, how many apples do you have left? (Just a gentle nudge to get us talking...)\"}, {'role': 'user', 'content': 'mesage 9'}, {'role': 'assistant', 'content': \"We've reached message 9. At this rate, I'm starting to think we might break the world record for most consecutive numbered messages. But enough about records, let's focus on having some fun. Since you haven't responded yet, I'll share a little joke to lighten the mood. Why couldn't the bicycle stand up by itself? (think about it for a sec...) Because it was two-tired! Okay, maybe that one was a bit of a groaner. But I'm here, ready to keep the conversation going whenever you're ready to start responding.\"}]\n",
            "2024-09-30 09:30:48 - Translated markdown file for en-US not found. Defaulting to chainlit.md.\n",
            "2024-09-30 09:30:52 - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "3 [{'role': 'system', 'content': 'You are a helpful assistant!'}, {'role': 'user', 'content': 'hi'}, {'role': 'assistant', 'content': \"How can I help you today? Do you have a question or something on your mind that you'd like to chat about?\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $PICKED_PORT\n",
        "!nohup chainlit run --port $PICKED_PORT app.py &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1opmqb2hZkV",
        "outputId": "bbed6cc6-1a5f-42f8-fc67-eb10cd3bf7cc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39651\n",
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat nohup.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PyyshswiJ9K",
        "outputId": "69b7a012-2b59-4897-8943-495955908ceb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-09-30 10:44:55 - Your app is available at http://localhost:45793\n",
            "2024-09-30 10:45:29 - Translated markdown file for en-US not found. Defaulting to chainlit.md.\n",
            "2024-09-30 10:46:02 - Translated markdown file for en-US not found. Defaulting to chainlit.md.\n",
            "2024-09-30 10:46:13 - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "3 [{'role': 'system', 'content': 'You are a helpful assistant!'}, {'role': 'user', 'content': 'hello'}, {'role': 'assistant', 'content': 'Hello. How can I assist you today?'}]\n",
            "2024-09-30 10:46:43 - Translated markdown file for en-US not found. Defaulting to chainlit.md.\n",
            "2024-09-30 10:47:10 - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2024-09-30 10:50:07 - Your app is available at http://localhost:39651\n",
            "2024-09-30 10:50:17 - Translated markdown file for en-US not found. Defaulting to chainlit.md.\n",
            "2024-09-30 10:50:23 - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "3 [{'role': 'system', 'content': 'You are a helpful assistant!'}, {'role': 'user', 'content': 'hello'}, {'role': 'assistant', 'content': \"Hello. It's nice to meet you. Is there something I can help you with or would you like to chat?\"}]\n",
            "2024-09-30 10:50:52 - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "5 [{'role': 'system', 'content': 'You are a helpful assistant!'}, {'role': 'user', 'content': 'hello'}, {'role': 'assistant', 'content': \"Hello. It's nice to meet you. Is there something I can help you with or would you like to chat?\"}, {'role': 'user', 'content': 'hi, generate  a joke'}, {'role': 'assistant', 'content': \"Why couldn't the bicycle stand up by itself?\\n\\n(Wait for it...)\\n\\nBecause it was two-tired.\"}]\n",
            "2024-09-30 10:51:53 - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import IFrame\n",
        "\n",
        "IFrame(src=external_url,width=\"100%\",height=640)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "id": "bT9q3oFggotH",
        "outputId": "92838272-892b-43bc-9892-591f3cfa4eb5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7f9cc4b28a60>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"100%\"\n",
              "            height=\"640\"\n",
              "            src=\"https://dt0fe3mg09w-496ff2e9c6d22116-39651-colab.googleusercontent.com/\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}